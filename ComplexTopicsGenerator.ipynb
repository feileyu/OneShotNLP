{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word(word_length):\n",
    "    word_len = np.random.randint(*word_length)\n",
    "    return ''.join(random.sample(string.ascii_lowercase, word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import MentionsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import MentionsLoader\n",
    "def transform_to_relations(file):\n",
    "    loader = MentionsLoader(filename=file, read_size=500,batch_size=2000, dict_size=None, tokenizer=None, ngrams_flag=None)\n",
    "    with open(file + '.rels', 'w') as out:\n",
    "        for batch in loader.iter_pairs_batch():\n",
    "            a, b, match = batch\n",
    "            target = ((np.array(match) + 1) // 2).astype(int)\n",
    "            for label, sa, sb in zip(target, a, b):\n",
    "                out.write(\"{}\\t{}\\t{}\\n\".format(label, sa, sb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTopicGenerator:\n",
    "    def __init__(self, num_topics, num_words_per_topic, word_length):\n",
    "        self.topics = []\n",
    "        for i in range(num_topics):\n",
    "            source = []\n",
    "            word_count = np.random.randint(*num_words_per_topic)\n",
    "            for j in range(word_count):\n",
    "                source.append(gen_word(word_length))\n",
    "            self.topics.append(source)\n",
    "    \n",
    "    def get_words(self, topic_id):\n",
    "        return random.sample(self.topics[topic_id], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopicGenerator(SimpleTopicGenerator):\n",
    "    \n",
    "    def get_words(self, topic_id):\n",
    "        noise_topic = random.randint(0, len(self.topics) - 1)\n",
    "        return random.sample(self.topics[topic_id], 1) + random.sample(self.topics[noise_topic], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicCoocGenerator:\n",
    "    \n",
    "    def __init__(self, num_topics, num_sources, num_words_per_source, word_length):\n",
    "        self.sources = []\n",
    "        for i in range(num_sources):\n",
    "            source = []\n",
    "            word_count = np.random.randint(*num_words_per_source)\n",
    "            for j in range(word_count):\n",
    "                source.append(gen_word(word_length))\n",
    "            self.sources.append(source)\n",
    "            \n",
    "        self.topics = sorted(list(set([\n",
    "            tuple(sorted(random.sample(range(0, num_sources), 2))) for _ in range(num_topics * 10)\n",
    "        ]))[:num_topics])\n",
    "        \n",
    "    def get_words(self, topic_id):\n",
    "        source_ids = self.topics[topic_id]\n",
    "        out = []\n",
    "        for source_id in source_ids:\n",
    "            out += random.sample(self.sources[source_id], 1)\n",
    "        random.shuffle(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicWordPrefix(TopicCoocGenerator):\n",
    "    \n",
    "    def augment_word(self, word):\n",
    "        prefix_len = np.random.randint(*(0, 3))\n",
    "        prefix = ''.join(random.sample(string.ascii_lowercase, prefix_len))\n",
    "        suffix_len = np.random.randint(*(0, 3))\n",
    "        suffix = ''.join(random.sample(string.ascii_lowercase, suffix_len))\n",
    "        return prefix + word + suffix\n",
    "        \n",
    "    \n",
    "    def get_words(self, topic_id):\n",
    "        out = list(map(self.augment_word, super(TopicWordPrefix, self).get_words(topic_id)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicNgramGenerator(TopicCoocGenerator):\n",
    "    \n",
    "    def get_words(self, topic_id):\n",
    "        out = super(TopicNgramGenerator, self).get_words(topic_id)\n",
    "        return [\" \".join(out)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    output,\n",
    "    topic_generator,\n",
    "    topics_count,\n",
    "    entities_count,\n",
    "    instance_per_entiry,\n",
    "    word_length,\n",
    "    topics_in_entity,\n",
    "    topic_words_per_entity,\n",
    "    random_words_per_entity,\n",
    "    random_words_count\n",
    "    ):\n",
    "    train_size = entities_count * instance_per_entiry // 4 * 3\n",
    "    \n",
    "    random_words = [gen_word(word_length) for _ in range(random_words_count)]\n",
    "    \n",
    "    entity_to_topics = []\n",
    "    lines = []\n",
    "    \n",
    "    for entity in range(entities_count):\n",
    "        entity_topics_count = np.random.randint(*topics_in_entity)\n",
    "        entity_topics = random.sample(range(topics_count), k=entity_topics_count)\n",
    "\n",
    "        entity_to_topics.append(entity_topics)\n",
    "\n",
    "        for i in range(instance_per_entiry):\n",
    "            words = []\n",
    "            entity_topic_words_count = np.random.randint(*topic_words_per_entity)\n",
    "            for w in range(entity_topic_words_count):\n",
    "                topic_id = random.choice(entity_topics)\n",
    "                words += topic_generator.get_words(topic_id)\n",
    "\n",
    "            random_words_count = np.random.randint(*random_words_per_entity)\n",
    "            words += random.sample(random_words, random_words_count)\n",
    "            random.shuffle(words)\n",
    "            mid = np.random.randint(0, len(words))\n",
    "\n",
    "            lines.append('{}\\t'.format(entity) +  ' '.join(words[:mid]) + \"\\t{}\\t\".format(entity) + ' '.join(words[mid:]) + '\\n')\n",
    "\n",
    "    with open(output + '_train.tsv', 'w') as fd:\n",
    "        for line in lines[:train_size]:\n",
    "            fd.write(line)\n",
    "            \n",
    "    with open(output + '_valid.tsv', 'w') as fd:\n",
    "        for line in lines[train_size:]:\n",
    "            fd.write(line)\n",
    "            \n",
    "    transform_to_relations(output + '_train.tsv')\n",
    "    transform_to_relations(output + '_valid.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = TopicCoocGenerator(\n",
    "        num_topics=10,\n",
    "        num_sources=5,\n",
    "        num_words_per_source=(1, 4),\n",
    "        word_length = (4, 7)\n",
    "    )\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_7',\n",
    "    topic_generator,\n",
    "    topics_count = 6,\n",
    "    entities_count = 200,\n",
    "    instance_per_entiry = 5,\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (1, 4),  # up to 3 random words\n",
    "    random_words_count = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_8',\n",
    "    topic_generator = TopicNgramGenerator(\n",
    "        num_topics=25,\n",
    "        num_sources=15,\n",
    "        num_words_per_source=(1, 40),\n",
    "        word_length = (4, 7)\n",
    "    ),\n",
    "    topics_count = 25,\n",
    "    entities_count = 10000,\n",
    "    instance_per_entiry = 5,\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (1, 4),  # up to 3 random words\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = TopicWordPrefix(\n",
    "        num_topics=25,\n",
    "        num_sources=15,\n",
    "        num_words_per_source=(1, 40),\n",
    "        word_length = (4, 7)\n",
    ")\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_9',\n",
    "    topic_generator=topic_generator,\n",
    "    topics_count = 25,\n",
    "    entities_count = 10000,\n",
    "    instance_per_entiry = 5,\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (1, 4),  # up to 3 random words\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = SimpleTopicGenerator(\n",
    "        num_topics=100,\n",
    "        num_words_per_topic=(1, 2),\n",
    "        word_length = (4, 7)\n",
    ")\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_10',\n",
    "    topic_generator=topic_generator,\n",
    "    topics_count = 100   ,\n",
    "    entities_count = 1000,\n",
    "    instance_per_entiry = 5, \n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (1, 2),\n",
    "    random_words_count = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = SimpleTopicGenerator(\n",
    "        num_topics=50,\n",
    "        num_words_per_topic=(1, 5), # More words per topic\n",
    "        word_length = (4, 7)\n",
    ")\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_11',\n",
    "    topic_generator=topic_generator,\n",
    "    topics_count = 50   ,\n",
    "    entities_count = 1000,\n",
    "    instance_per_entiry = 5, \n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (1, 2),\n",
    "    random_words_count = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = NoisyTopicGenerator(\n",
    "        num_topics=100,\n",
    "        num_words_per_topic=(1, 2),\n",
    "        word_length = (4, 7)\n",
    ")\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_12',\n",
    "    topic_generator=topic_generator,\n",
    "    topics_count = 100   ,\n",
    "    entities_count = 1000,\n",
    "    instance_per_entiry = 5, \n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (1, 2),\n",
    "    random_words_count = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_generator = NoisyTopicGenerator(\n",
    "        num_topics=400,\n",
    "        num_words_per_topic=(1, 5),\n",
    "        word_length = (4, 7)\n",
    ")\n",
    "\n",
    "generate(\n",
    "    './data/debug_data/syntetic_13',\n",
    "    topic_generator=topic_generator,\n",
    "    topics_count = 400   ,\n",
    "    entities_count = 10000,\n",
    "    instance_per_entiry = 5, \n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (1, 2),\n",
    "    random_words_count = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
