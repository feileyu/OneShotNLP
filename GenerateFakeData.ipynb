{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word():\n",
    "    word_len = np.random.randint(*word_length)\n",
    "    return ''.join(random.sample(string.ascii_lowercase, word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import MentionsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import MentionsLoader\n",
    "def transform_to_relations(file):\n",
    "    loader = MentionsLoader(filename=file, read_size=500,batch_size=2000, dict_size=None, tokenizer=None)\n",
    "    with open(file + '.rels', 'w') as out:\n",
    "        for batch in loader.iter_pairs_batch():\n",
    "            a, b, match = batch\n",
    "            target = ((np.array(match) + 1) // 2).astype(int)\n",
    "            for label, sa, sb in zip(target, a, b):\n",
    "                out.write(\"{}\\t{}\\t{}\\n\".format(label, sa, sb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    output,\n",
    "    topics_count,\n",
    "    entities_count,\n",
    "    instance_per_entiry,\n",
    "    words_in_topic,\n",
    "    word_length,\n",
    "    topics_in_entity,\n",
    "    topic_words_per_entity,\n",
    "    random_words_per_entity,\n",
    "    random_words_count\n",
    "    ):\n",
    "    train_size = entities_count * instance_per_entiry // 4 * 3,\n",
    "    topics = []\n",
    "    for i in range(topics_count):\n",
    "        topic = []\n",
    "        word_count = np.random.randint(*words_in_topic)\n",
    "        for j in range(word_count):\n",
    "            topic.append(gen_word())\n",
    "        topics.append(topic)\n",
    "\n",
    "    random_words = [gen_word() for _ in range(random_words_count)]\n",
    "\n",
    "    entity_to_topics = []\n",
    "    lines = []\n",
    "    \n",
    "    for entity in range(entities_count):\n",
    "        entity_topics_count = np.random.randint(*topics_in_entity)\n",
    "        entity_topics = random.sample(range(topics_count), k=entity_topics_count)\n",
    "\n",
    "        entity_to_topics.append(entity_topics)\n",
    "\n",
    "        for i in range(instance_per_entiry):\n",
    "            words = []\n",
    "            entity_topic_words_count = np.random.randint(*topic_words_per_entity)\n",
    "            for w in range(entity_topic_words_count):\n",
    "                topic_id = random.choice(entity_topics)\n",
    "                words.append(random.choice(topics[topic_id]))\n",
    "\n",
    "            random_words_count = np.random.randint(*random_words_per_entity)\n",
    "            for r in range(random_words_count):\n",
    "                words.append(random.choice(random_words))\n",
    "\n",
    "            random.shuffle(words)\n",
    "\n",
    "            mid = np.random.randint(0, len(words))\n",
    "\n",
    "            lines.append('{}\\t'.format(entity) +  ' '.join(words[:mid]) + \"\\t{}\\t\".format(entity) + ' '.join(words[mid:]) + '\\n')\n",
    "\n",
    "    with open(output + '_train.tsv', 'w') as fd:\n",
    "        for line in lines[:train_size]:\n",
    "            fd.write(line)\n",
    "            \n",
    "    with open(output + '_valid.tsv', 'w') as fd:\n",
    "        for line in lines[train_size:]:\n",
    "            fd.write(line)\n",
    "            \n",
    "    transform_to_relations(output + '_train.tsv')\n",
    "    transform_to_relations(output + '_valid.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_1',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 2,\n",
    "    words_in_topic = (1,2),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (0, 1),\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_2',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,10),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 2),\n",
    "    random_words_per_entity = (0, 1),\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_3',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,10),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (0, 1),\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_3.5',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,40), # More!\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (0, 1),\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_4',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,10),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 4), # More topics per entity!\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (0, 1),\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_5',\n",
    "    topics_count = 25,\n",
    "    entities_count = 2000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,40),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (1, 2), # One random word\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    './data/debug_data/syntetic_6',\n",
    "    topics_count = 25,\n",
    "    entities_count = 10000,\n",
    "    instance_per_entiry = 5,\n",
    "    words_in_topic = (1,40),\n",
    "    word_length = (4, 7),\n",
    "    topics_in_entity = (1, 2),\n",
    "    topic_words_per_entity = (1, 4),\n",
    "    random_words_per_entity = (1, 4),  # up to 3 random words\n",
    "    random_words_count = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
